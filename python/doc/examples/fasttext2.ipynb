{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MultiLabel-Text-Classification-with-FastText2\" data-toc-modified-id=\"MultiLabel-Text-Classification-with-FastText2-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MultiLabel Text Classification with FastText2</a></span><ul class=\"toc-item\"><li><span><a href=\"#BackGround\" data-toc-modified-id=\"BackGround-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>BackGround</a></span></li><li><span><a href=\"#Quick-Introduction-to-Fasttext\" data-toc-modified-id=\"Quick-Introduction-to-Fasttext-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Quick Introduction to Fasttext</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Model Training</a></span></li></ul></li><li><span><a href=\"#Fasttext2-Inferencing-Benchmark\" data-toc-modified-id=\"Fasttext2-Inferencing-Benchmark-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Fasttext2 Inferencing Benchmark</a></span></li><li><span><a href=\"#Fasttext2-Modification\" data-toc-modified-id=\"Fasttext2-Modification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fasttext2 Modification</a></span></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2020-05-07 13:31:48 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 7.9.0\n",
      "\n",
      "numpy 1.16.5\n",
      "pandas 0.25.0\n"
     ]
    }
   ],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# notice the import is from fasttext2\n",
    "# can be installed with python setup.py install,\n",
    "# this package is not on pip and as the package is\n",
    "# renamed to fasttext2 it does not conflict with\n",
    "# the original fasttext package\n",
    "import fasttext2 as fasttext\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLabel Text Classification with FastText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackGround"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi label classification is different from regular classification task where there is single ground truth that we are predicting. Here, each record can have multiple labels attached to it. e.g. in the data that we'll be working with later, our goal is to build a classifier that assigns tags to stackexchange questions about cooking. As we can imagine, each question can belong into multiple tags/topics at the same time, i.e. each record have multiple \"correct\" labels/targets. Let's look at some examples to materialize this.\n",
    "\n",
    "```\n",
    "__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\n",
    "__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments\n",
    "__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?\n",
    "```\n",
    "\n",
    "Looking at the first few lines, we can see that for each question, its corresponding tags are prepended with the `__label__` prefix. Our task is to train a model that predicts the tags/labels given the question.\n",
    "\n",
    "This file format is expected by [Fasttext](https://fasttext.cc/), the library we'll be using to train our tag classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Introduction to Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Fasttext to train our text classifier. Fasttext at its core is composed of two main idea.\n",
    "\n",
    "First, unlike deep learning methods where there are multiple hidden layers, the architecture is similar to Word2vec. After feeding the words into 1 hidden layer, the words representation are averaged into the sentence representation and directly followed by the output layer.\n",
    "\n",
    "<img src=\"img/fasttext.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "This seemingly simple method works extremely well on classification task, and from the original paper it can achieve performance that are on par with more complex deep learning methods, while being significantly quicker to train.\n",
    "\n",
    "The second idea is instead of treating words as the basic entity, it uses character n-grams or word n-grams as additional features. For example, in the sentence, \"I like apple\", the 1-grams are 'I', 'like', 'apple'. The word 2-gram are consecutive word such as: 'I like', 'like apple', whereas the character 2-grams are for the word apple are 'ap', 'pp', 'pl', 'le'. By using word n-grams, the model now has the potential to capture some information from the ordering of the word. Whereas, with character n-grams, the model can now generate better embeddings for rare words or even out of vocabulary words as we can compose the embedding for a word using the sum or average of its character n-grams.\n",
    "\n",
    "For readers accustomed to Word2vec, one should note that the word embedding/representation for the classification task is neither the skipgram or cbow method. Instead it is tailored for the classification task at hand. To elaborate:\n",
    "\n",
    "- Given a word, predict me which other words should go around (skipgram).\n",
    "- Given a sentence with a missing word, find me the missing word (cbow).\n",
    "- Given a sentence, tell me which label corresponds to this sentence (classification).\n",
    "\n",
    "Hence, for skipgram and cbow, words in the same context will tend to have their word embedding/representation close to each other. As for classification task, words that are most discriminative for a given label will be close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll download the data and take a peek at it. Then it's the standard train and test split on our text file. As Fasttext accepts the input data as files, the following code chunk provides a function to perform the split without reading all the data into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-07 13:31:49--  https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 457609 (447K) [application/x-tar]\n",
      "Saving to: ‘data/cooking.stackexchange.tar.gz’\n",
      "\n",
      "cooking.stackexchan 100%[===================>] 446.88K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-05-07 13:31:49 (4.28 MB/s) - ‘data/cooking.stackexchange.tar.gz’ saved [457609/457609]\n",
      "\n",
      "x cooking.stackexchange.id\n",
      "x cooking.stackexchange.txt\n",
      "x readme.txt\n"
     ]
    }
   ],
   "source": [
    "# download the data and un-tar it under the 'data' folder\n",
    "\n",
    "# -P or --directory-prefix specifies which directory to download the data to\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz -P data\n",
    "# -C specifies the target directory to extract an archive to\n",
    "!tar xvzf data/cooking.stackexchange.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?\r\n",
      "__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments\r\n",
      "__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 data/cooking.stackexchange.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def train_test_split_file(input_path: str,\n",
    "                          output_path_train: str,\n",
    "                          output_path_test: str,\n",
    "                          test_size: float,\n",
    "                          random_state: int=1234,\n",
    "                          encoding: str='utf-8',\n",
    "                          verbose: bool=True):\n",
    "    random.seed(random_state)\n",
    "\n",
    "    # we record the number of data in the training and test\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    train_range = 1 - test_size\n",
    "\n",
    "    with open(input_path, encoding=encoding) as f_in, \\\n",
    "         open(output_path_train, 'w', encoding=encoding) as f_train, \\\n",
    "         open(output_path_test, 'w', encoding=encoding) as f_test:\n",
    "\n",
    "        for line in f_in:\n",
    "            random_num = random.random()\n",
    "            if random_num < train_range:\n",
    "                f_train.write(line)\n",
    "                count_train += 1\n",
    "            else:\n",
    "                f_test.write(line)\n",
    "                count_test += 1\n",
    "\n",
    "    if verbose:\n",
    "        print('train size: ', count_train)\n",
    "        print('test size: ', count_test)\n",
    "\n",
    "\n",
    "def prepend_file_name(path: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    e.g. data/cooking.stackexchange.txt\n",
    "    prepend 'train' to the base file name\n",
    "    data/train_cooking.stackexchange.txt\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(path)\n",
    "    file_name = os.path.basename(path)\n",
    "    return os.path.join(directory, name + '_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  12297\n",
      "test size:  3107\n",
      "train path:  data/train_cooking.stackexchange.txt\n",
      "test path:  data/test_cooking.stackexchange.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "test_size = 0.2\n",
    "input_path = os.path.join(data_dir, 'cooking.stackexchange.txt')\n",
    "input_path_train = prepend_file_name(input_path, 'train')\n",
    "input_path_test = prepend_file_name(input_path, 'test')\n",
    "random_state = 1234\n",
    "encoding = 'utf-8'\n",
    "\n",
    "train_test_split_file(input_path, input_path_train, input_path_test,\n",
    "                      test_size, random_state, encoding)\n",
    "print('train path: ', input_path_train)\n",
    "print('test path: ', input_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to the full list of parameters from [Fasttext's documentation page](https://fasttext.cc/docs/en/python-module.html#train_supervised-parameters). Like with all machine learning models, feel free to experiment with various hyperparameters, and see which one leads to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  14496\n",
      "label size:  733\n",
      "example vocab:  ['</s>', 'to', 'a', 'How', 'the']\n",
      "example label:  ['__label__baking', '__label__food-safety', '__label__substitutions', '__label__equipment', '__label__bread']\n"
     ]
    }
   ],
   "source": [
    "# lr = learning rate\n",
    "# lrUpdateRate similar to batch size\n",
    "fasttext_params = {\n",
    "    'input': input_path_train,\n",
    "    'lr': 0.1,\n",
    "    'lrUpdateRate': 1000,\n",
    "    'thread': 8,\n",
    "    'epoch': 10,\n",
    "    'wordNgrams': 1,\n",
    "    'dim': 100,\n",
    "    'loss': 'ova'\n",
    "}\n",
    "model = fasttext.train_supervised(**fasttext_params)\n",
    "\n",
    "print('vocab size: ', len(model.words))\n",
    "print('label size: ', len(model.labels))\n",
    "print('example vocab: ', model.words[:5])\n",
    "print('example label: ', model.labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not used here, fasttext has a parameter called `bucket`. It can be a bit unintuitive what the parameter controls. We note down the [explanation provided by the package maintainer](https://github.com/facebookresearch/fastText/issues/641).\n",
    "\n",
    "> The size of the model will increase linearly with the number of buckets. The size of the input matrix is DIM x (VS + BS), where VS is the number of words in the vocabulary and BS is the number of buckets. The number of buckets does not have other influence on the model size.\n",
    "> The buckets are used for hashed features (such as character ngrams or word ngrams), which are used in addition to word features. In the input matrix, each word is represented by a vector, and the additional ngram features are represented by a fixed number of vectors (which corresponds to the number of buckets).\n",
    "\n",
    "The loss function that we've specified is one versus all, `ova` for short. This type of loss function handles the multiple labels by building independent binary classifiers for each label.\n",
    "\n",
    "Upon training the model, we can take a look at the prediction generated by the model via passing a question to the `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__sauce', '__label__cheese'), array([0.77185351, 0.53899324]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'How much does potato starch affect a cheese sauce recipe?'\n",
    "model.predict(text, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated tags for this question were `__label__sauce` and `__label__cheese`. Meaning we got both the prediction correct when asking for the top 2 tags. i.e. the precision@2 (precision at 2) for this example is 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__food-safety', '__label__storage-method'),\n",
       " array([0.21207881, 0.06561483]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Dangerous pathogens capable of growing in acidic environments'\n",
    "model.predict(text, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the annotated tags were `__label__food-safety` and `__label__storage-method`. In other words, 1 of our predicted tag was wrong, hence the precision@2 is 50%.\n",
    "\n",
    "Notice the second prediction's score is pretty low, when calling the `.predict` method, we can also provide a threshold to cutoff predictions lower than that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__food-safety',), array([0.21207881]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Dangerous pathogens capable of growing in acidic environments'\n",
    "model.predict(text, k=2, threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.predict` method also supports batch prediction, where we pass in a list of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['__label__sauce', '__label__cheese'],\n",
       "  ['__label__food-safety', '__label__storage-method']],\n",
       " [array([0.7718535 , 0.53899324], dtype=float32),\n",
       "  array([0.21207881, 0.06561483], dtype=float32)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'How much does potato starch affect a cheese sauce recipe?',\n",
    "    'Dangerous pathogens capable of growing in acidic environments'\n",
    "]\n",
    "batch_results = model.predict(texts, k=2)\n",
    "batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the prediction, we might want to clean up the label prediction such as removing the `__label__` indicator that fasttext uses to differentiate which token is a label and which is a input word/token. Also replace the dashed `-` in between each token with a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_LABEL = '__label__'\n",
    "\n",
    "\n",
    "def parse_fasttext_label(label):\n",
    "    return label[len(FASTTEXT_LABEL):].replace('-', ' ')\n",
    "\n",
    "\n",
    "def parse_batch_labels(batch_labels):\n",
    "    parsed_batch_labels = []\n",
    "    for labels in batch_labels:\n",
    "        parsed_labels = [parse_fasttext_label(label) for label in labels]\n",
    "        parsed_batch_labels.append(parsed_labels)\n",
    "\n",
    "    return parsed_batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sauce', 'cheese'], ['food safety', 'storage method']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the result from the .predict method is a tuple of label and score\n",
    "# we only need to parse the label\n",
    "parse_batch_labels(batch_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the precision/recall metrics all together on our train and test file, we can leverage the `.test` method from the model to evaluate the overall precision and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model, input_path, k):\n",
    "    num_records, precision_at_k, recall_at_k = model.test(input_path, k)\n",
    "    f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "\n",
    "    print(\"records\\t{}\".format(num_records))\n",
    "    print(\"Precision@{}\\t{:.3f}\".format(k, precision_at_k))\n",
    "    print(\"Recall@{}\\t{:.3f}\".format(k, recall_at_k))\n",
    "    print(\"F1@{}\\t{:.3f}\".format(k, f1_at_k))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics:\n",
      "records\t12297\n",
      "Precision@1\t0.485\n",
      "Recall@1\t0.211\n",
      "F1@1\t0.294\n",
      "\n",
      "test metrics:\n",
      "records\t3107\n",
      "Precision@1\t0.410\n",
      "Recall@1\t0.177\n",
      "F1@1\t0.247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "print('train metrics:')\n",
    "print_results(model, input_path_train, k)\n",
    "\n",
    "print('test metrics:')\n",
    "print_results(model, input_path_test, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the model under its own folder/directory\n",
    "directory = 'cooking_model'\n",
    "if not os.path.isdir(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "model_checkpoint = os.path.join(directory, 'fasttext2_model.fasttext')\n",
    "model.save_model(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext2 Inferencing Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything that's shown above is provided by the original `fasttext` library, `fasttext2` mainly added code to speed up model inferencing, i.e. after training the model, generating prediction for new inputs. The core logic that does the model training remains the same.\n",
    "\n",
    "Here, we benchmark the different predict methods. We fix the top-k labels we're predicting and the batch size (The number of input text that we wish to get the output for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_read_text(input_path: str, batch_size: int=500, encoding: str='utf-8'):\n",
    "    texts = []\n",
    "    with open(input_path, encoding=encoding) as f:\n",
    "        for _ in range(batch_size):\n",
    "            try:\n",
    "                tokens = []\n",
    "                line = f.readline().strip('\\n')\n",
    "                for token in line.split(' '):\n",
    "                    if FASTTEXT_LABEL not in token:\n",
    "                        tokens.append(token)\n",
    "\n",
    "                text = ' '.join(tokens)\n",
    "                texts.append(text)\n",
    "            except ValueError as e:\n",
    "                # bad practice to just skip exceptions,\n",
    "                # we'll let it slide for this demo code ...\n",
    "                continue\n",
    "                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dangerous pathogens capable of growing in acidic environments',\n",
       " 'How do I cover up the white spots on my cast iron stove?',\n",
       " \"What's the purpose of a bread box?\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_texts = batch_read_text(input_path_train, batch_size)\n",
    "print('batch size: ', len(batch_texts))\n",
    "batch_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original `.predict` method from fasttext library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.4 ms ± 1.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_results = model.predict(batch_texts, k=k)\n",
    "batch_labels = parse_batch_labels(batch_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want the predicted label, and not the score, we can use `.predict_label`. The speed gain here is not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_labels = model.predict_label(batch_texts, k=k)\n",
    "batch_labels = parse_batch_labels(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasttext uses sequential loop to generate prediction for a batch of text. One way to improve the inferencing speed is to change to a parallel loop to generate the prediction. This is exposed using the `predict_label_future` method. It uses c++'s async/future pattern to do the parallelize, hence the future naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1 ms ± 480 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_labels = model.predict_label_future(batch_texts, k=k)\n",
    "batch_labels = parse_batch_labels(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind next speed up is not as straightforward as implementing parallel prediction on top of the sequential prediction. \n",
    "\n",
    "At a high level, it uses `hnswlib` to create an index on the output matrix of the model. Doing this indexing allows us the speed up looking for the top-k predicted labels for all future predictions. We'll defer the elaborated explanation till later, and see how to leverage it as an end-user first.\n",
    "\n",
    "We need to create an index using the `create_index` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fasttext2.FastText._FastText at 0x11e9bdbe0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_params = {\n",
    "    'ef_construction': 100,\n",
    "    'M': 5,\n",
    "    'random_seed': 100\n",
    "}\n",
    "model.create_index(**index_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we call `batch_predict_label` to perform the batch prediction. Note that multi-label classification problem that has a large label space will see more gains with this \"indexing\" trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 714 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_labels = model.batch_predict_label(batch_texts, k=k)\n",
    "batch_labels = parse_batch_labels(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the predicted labels to make sure the prediction remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__sauce', '__label__cheese', '__label__tomatoes'],\n",
       " ['__label__food-safety']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "batch_labels = model.predict_label_future(texts, k=k, threshold=threshold)\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__sauce', '__label__cheese', '__label__tomatoes'],\n",
       " ['__label__food-safety']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels = model.batch_predict_label(texts, k=k, threshold=threshold)\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the \"indexed model\" using the original `save_model` method, and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_model_checkpoint = os.path.join(directory, 'fasttext2_indexed_model.fasttext')\n",
    "model.save_model(indexed_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['__label__sauce',\n",
       "  '__label__cheese',\n",
       "  '__label__tomatoes',\n",
       "  '__label__pasta',\n",
       "  '__label__flavor'],\n",
       " ['__label__food-safety',\n",
       "  '__label__storage-method',\n",
       "  '__label__storage-lifetime',\n",
       "  '__label__food-science',\n",
       "  '__label__refrigerator']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model = fasttext.load_model(indexed_model_checkpoint)\n",
    "\n",
    "# confirm prediction still works\n",
    "batch_labels = fasttext_model.batch_predict_label(texts, k=k)\n",
    "batch_labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of section demonstrates we can use the quantization capability provided by fasttext and the indexing trick together to both speed up inferencing and reduce memory reduction at the same time with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# we load the model\n",
    "fasttext_model = fasttext.load_model(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the original .quantize method to quantize the model\n",
    "fasttext_model.quantize(dsub=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fasttext2.FastText._FastText at 0x12642d748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates the index\n",
    "index_params = {\n",
    "    'ef_construction': 100,\n",
    "    'M': 5,\n",
    "    'random_seed': 100\n",
    "}\n",
    "fasttext_model.create_index(**index_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__sauce', '__label__cheese', '__label__pasta', '__label__tomatoes'],\n",
       " ['__label__food-safety']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the prediction\n",
    "batch_labels = fasttext_model.batch_predict_label(texts, k=k, threshold=threshold)\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the quantized and indexed model\n",
    "quantized_model_checkpoint = os.path.join(directory, 'fasttext2_quantized_model.fasttext')\n",
    "fasttext_model.save_model(quantized_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load the quantized and indexed model back into memory\n",
    "fasttext_model = fasttext.load_model(quantized_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28904\r\n",
      "-rw-r--r--  1 mingyuliu  110304721   6.4M May  7 13:32 fasttext2_indexed_model.fasttext\r\n",
      "-rw-r--r--  1 mingyuliu  110304721   6.1M May  7 13:31 fasttext2_model.fasttext\r\n",
      "-rw-r--r--  1 mingyuliu  110304721   1.6M May  7 13:32 fasttext2_quantized_model.fasttext\r\n"
     ]
    }
   ],
   "source": [
    "# check the size of each model\n",
    "!ls -lh cooking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__sauce', '__label__cheese', '__label__pasta', '__label__tomatoes'],\n",
       " ['__label__food-safety']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the prediction\n",
    "batch_labels = fasttext_model.batch_predict_label(texts, k=k, threshold=threshold)\n",
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 µs ± 2.44 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "batch_labels = fasttext_model.predict_label_future(texts, k=5)\n",
    "batch_labels = parse_batch_labels(batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext2 Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We listed the major code change to the original Fasttext source code.\n",
    "\n",
    "- We switched the package name from `fasttext` to `fasttext2` to prevent over-stepping on each other. Note that during the import statement, we can use `import fasttext2 as fasttext` to avoid changes to other parts of the code. As this work is mainly around speeding inferencing, all the other commonly used methods, such as `train_supervised`, `save_model`, `load_model`, `quantize` will work as usual.\n",
    "- The fasttext object from `fasttext2` mainly adds `create_index`, `batch_predict_label`, `predict_label_future` method to speed up inferencing.\n",
    "- For the underlying C++ code:\n",
    "    - In the main `FastText` class, there is a `Model` class that takes care of the computational aspect, `Loss` class that handles loss and applies gradients to the output. Here, we refactor `Model`'s `computeHidden` method so instead of having a method that directly computes the hidden layer and output layer in one go, we isolate the `computeHidden` this gives us the flexibility to only compute up to the hidden layer, so we can redirect rest of the inferencing to the faster indexing method. Also the `Loss` contains a method that computes the `sigmoid` given an input number, we also expose that so we can re-use it outside the `Loss` context.\n",
    "    - We add the `Index` class that contains the core logic to create the hnsw index, also perform knn search from the index. The `Index` class is a wrapper around the `hnswlib` library. We also include this object to the `FastText` class. The `Index` class can be created using the newly added `createIndex` method of the `FastText` class. There is also a `indexed_` attribute, similar to the `quant_` attribute that is used to indicate whether we quantize the model, we added this attribute to denote whether the model is indexed. The `Index` has a `save` and `load` method that works with `FastText` object, i.e. we can save/load the `FastText` object using its `saveModel`, `loadModel` method, and it will streamline save/load the underlying `Index` together with the `FastText` object if we created one.\n",
    "    - The `knnQueryLineLabel` method is added to the `FastText` object to generate the prediction using the index.\n",
    "    - In the `fasttext2_pybind.cc` script that uses pybind11 to expose the C++ code into python. We added the methods to the `FastText` class. `multilinePredictLabelFuture` for performing parallel prediction using C++'s async/future pattern. The original `multilinePredict` method performs prediction sequentially for the batch of input text. `multilineKnnQueryLabel` is for performing parallel prediction after we've indexed the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Fasttext Documentation: Text Classification](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "- [Quora: What is the main difference between word2vec and fastText?](https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText)\n",
    "- [Paper: A. Joulin, E. Grave, P. Bojanowski, T. Mikolov - Bag of Tricks for Efficient Text Classification (2016)](https://arxiv.org/abs/1607.01759)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "197.587px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
